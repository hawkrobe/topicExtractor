// LDA
//
// Run this with:
// LOG_LEVEL=1 ./webppl examples/ldaVariational-ndg.wppl

// Known issues:

// It's not uncommon to see the following error:
// "AssertionError: Expected marginal to be normalized."

// I'm not sure what causes this, but one guess is that it's possible
// to sample values on the boundary of the simplex from the logistic
// normal. Such values are outside of the support of the Dirichlet, so
// scoring them may lead to the gradient blowing up.



/*
comments on variational from my explorations (these belong somewhere else):

-Q: if we use LR with differentiable sampler, do we get reparam? A: no.
-need to understand what trick yields LR from non-differentiable sampler.

-can we relax or enumerate all discrete vars, and then add discrete randomization only at conditionals? i.e. we should only have to incur the variance hit for structure change.
 -is the current method actually correct when there’s structure change?

-how can / should we use the VI guide for SMC? get guide params, use in withImportanceDist?
-maybe the core variational routine should just return a parameters / sampler object?
 -arguably variational is not a standalone inference method, but only finds guide params for use in importance sampling, SMC, etc.
-we should probably unify proposers (MH), importance distributions (SMC), and guides (Variational).

-should make better library for mini-batches / updates to parallel sub-models.
  -should allow random mini-batch as well as cycling.
  -the minibatch forEach should take batch size (rather than being global) to facilitate nested batching.

-what does .bind(this) do? it’s all over the code! there must be a clearer way to write these inference loops... macros to the rescue?

-for LDA:
 -need to fix the simplex-edge bug.
 -does the nerual guide work?
 -build the cocolab abstracts corpus!
   -`$(“.abstract”).text().split(“Abstract:”)` in the pubs page gets raw array of abstracts.
   -then need to tokenize and clean up…
   -throw out stop words and those with small count across corpus?

*/

// This seem to happen more often with larger step sizes.

var ones = function(dims) { zeros(dims).add(1); };

var forEach2 = function(f, xs) {
  map(function(i) {
    return f(xs[i], i);
  }, _.range(xs.length));
  return;
};

var paramInit = function(val, options) {
  return paramChoice(deltaERP, [val], options);
};

// Hack to call erp logistic without member expression. Useful for
// approximating dirichlet with delta...
var erp = { logistic: logistic };

var initSigma = 0.05;
var hDim = 10;

//TODO: do some profiling. why is this slow?

var lda = function(corpus, vocabSize, numTopics, alpha, eta) {

  var topics = repeat(numTopics, function() {
    var q = sampleGuide(logisticNormalERP, [
      paramInit(zeros([vocabSize - 1, 1]), { prefix: 'topic_mu' }),
      ad.tensor.exp(paramInit(zeros([vocabSize - 1, 1]), { prefix: 'topic_log_sigma' }))
    ]);

    return sample(dirichletERP, [eta], { guideVal: q }); // beta_k
  });

  //TODO: think about better init?
  var W0 = paramChoice(matrixGaussianERP, [0, initSigma, [hDim, vocabSize]], { name: 'W0' });
  var W1 = paramChoice(matrixGaussianERP, [0, initSigma, [numTopics - 1, hDim]], { name: 'W1' });
  var W2 = paramChoice(matrixGaussianERP, [0, initSigma, [numTopics - 1, hDim]], { name: 'W2' });
  var b0 = paramChoice(matrixGaussianERP, [0, initSigma, [hDim, 1]], { name: 'b0' });
  var b1 = paramChoice(matrixGaussianERP, [0, initSigma, [numTopics - 1, 1]], { name: 'b1' });
  var b2 = paramChoice(matrixGaussianERP, [0, initSigma, [numTopics - 1, 1]], { name: 'b2' });

  miniBatch(corpus, function(doc) {

    //neural guide: use a NN with the word-count vector for the document as input that spits out the props_mu and props_log_sigma for the logisticNormal guide.
    //a fancy modern way to do this woulld be to convert words to vectors via a (learned) embedding and then use an RNN to build the doc vector from which we get mu and sigma.... not sure this would actually do better than the count-vector method.
    var docTensor = ad.tensor.reshape({data:doc, length:doc.length},[vocabSize,1])
    var h = ad.tensor.tanh(ad.tensor.add(ad.tensor.dot(W0, docTensor), b0));
    var mu = ad.tensor.add(ad.tensor.dot(W1, h), b1);
    var log_sigma = ad.tensor.exp(ad.tensor.add(ad.tensor.dot(W2, h), b2));
    var q = sampleGuide(logisticNormalERP, [mu, ad.tensor.exp(log_sigma)]);

    // //version without the NN guide:
    // var q = sampleGuide(logisticNormalERP, [
    //   paramInit(zeros([numTopics - 1, 1]), { prefix: 'props_mu' }),
    //   ad.tensor.exp(paramInit(zeros([numTopics - 1, 1]), { prefix: 'props_log_sigma' }))
    // ]);

    var topicDist = sample(dirichletERP, [alpha], { guideVal: q }); // theta_d

    var marginal = Enumerate(function() {
      //TODO: what happens if we relax this? first write via one-hot repr for z, which turns index lookup into matrix multiply. Then relax the one-hot from discrete into a vec from dirichlet... 
      var z = sample(discreteERP, [topicDist]);
      var topic = topics[z];
      var word = sample(discreteERP, [topic]);
      return word;
    });

    // faster way to do this? only do factor if count!=0?
    map(function(i){factor(doc[i]*marginal.score([], i))}, wordIndices)

  }, {batchSize: 3, selectionMethod: 'random'});

  return topics;

};


// Each document is represented by an array of word counts. Therefore
// doc.length == vocabSize, and sum(doc) = no. of words in doc.

var corpus = readJSON('cocolabAbstractCorpusIndices.json');

//use count vectors for now...
var docs = corpus.documentsAsCounts

var vocabSize = docs[0].length; // V
var wordIndices = _.range(0,vocabSize)
var numTopics = 4; // K


// Parameter for prior on topic proportions.
var alpha = Vector(repeat(numTopics, constF(0.1)));
// Parameter for prior on topics.
var eta = Vector(repeat(vocabSize, constF(0.1)));

var marginal = Variational(function() {
    return lda(docs, vocabSize, numTopics, alpha, eta);
  }, {
  optimizer: { adagrad: { stepSize: 0.1 } },
  //optimizer: { adam: { stepSize: 0.01 } },
  steps: 2,
  samplesPerStep: 1,
  returnSamples: 1,
  callback: function(i, parameters) {
    var mus = [parameters.topic_mu0,
               parameters.topic_mu1,
               parameters.topic_mu2,
               parameters.topic_mu3];

    // var sigmas = [parameters.topic_log_sigma0,
    //               parameters.topic_log_sigma1,
    //               parameters.topic_log_sigma2,
    //               parameters.topic_log_sigma3];

    display(_.map(mus, logistic));
    //display(_.map(sigmas, ad.tensor.exp));
  }
});

//map(function(t) { return ad.value(t).toFlatArray(); }, marginal.support()[0]);
//writeJSON('topics.json', map(function(topic) { topic.toFlatArray(); }, marginal.support()[0]));

'done';
